{"componentChunkName":"component---src-templates-blog-list-tsx","path":"/blog/page/4","result":{"data":{"remark":{"posts":[{"post":{"html":"<p>Does the SAS DDS support Bitemporal historisation? Why yes indeed, with the right transform and extract logic, and updates to the primary keys.</p>\n<p>The SAS Detailed Data Store (DDS) suite provides \"out of the box\" data models for industries such as Banking, Insurance and Telco. They arrive with logical and physical models (eg in Erwin format), and a set of DDL files for your preferred database. These assets help to accelerate the delivery and deployment of a company data warehouse that has the further advantage of standardised integration with SAS Solution offerings. The model is also very large. It is highly likely that much of the model will be unused, especially in industries like insurance - where corporate structures, regulatory environments, and product offerings can be quite diverse.</p>\n<p>As a \"baseline\" model then, another way to utilise it is to take it as just that - a model, that provides guidance on putting together a warehouse that suits you, the customer. There is nothing to stop you picking &#x26; choosing the parts you like, that make sense for your particular use case(s).</p>\n<h2>The Validity of SCD2</h2>\n<p>The historisation approach in the DDS is based on the \"VALID_FROM_DTTM\" and \"VALID_TO_DTTM\" columns. These provide the open and close datetime pairs representing the 'validity' of the record. This - is where the confusion begins.</p>\n<p>What is 'validity'? Perhaps this represents the 'truth', eg the number of widgets we sold last month. So what is this truth? Is it the state of the database (transaction datetimes) between the 1st and the end of the month? Or does it represent the current view of last months widget sales, which were finally loaded on, say, the 15th of the following month? Dealing with this scenario (late arriving records) poses a number of challenges for the SCD2 model:</p>\n<ul>\n<li>Reloading historical data (as records must be physically removed, in order to reload)&#x3C;</li>\n<li>Loading corrections (again, records must first be removed)&#x3C;</li>\n<li>Maintaining an audit history, or even <em>the ability to run the same query twice and get the same result.</em></li>\n</ul>\n<p>An emerging consensus from the datawarehousing domain is the use of bitemporal datetime ranges for managing such requirements. The below article borrows heavily from <a href=\"/wp-content/uploads/2020/08/hist-op_1.1.6_en_manual.pdf\">this excellent paper</a> by <a href=\"https://www.linkedin.com/in/arnd-wussing-8660381\">Arnd Wussing</a>, which explains the topic in much greater detail.</p>\n<h2>Background to Bitemporal</h2>\n<p>The concept of Bi-Temporal Historisation is not new – it was originally associated with a chap called Richard Snodgrass back in 1992. <span style=\"color: #000000;\"><span>There is further info on </span></span><span style=\"color: #0000ff;\"><u><a href=\"http://en.wikipedia.org/wiki/Temporal_database\"><span>Wikipedia</span></a></u></span><span style=\"color: #000000;\"><span> and </span></span><span style=\"color: #0000ff;\"><u><a href=\"http://informix-myview.blogspot.co.uk/2012/03/bitemporal-data-is-this-next-big-thing.html\"><span>this blog</span></a></u></span><span style=\"color: #000000;\"><span>, and a more recent article on <a href=\"https://medium.com/kamu-data/a-brief-history-of-time-in-data-modelling-olap-systems-9032f63b8b7f\">medium</a>.</span></span></p>\n<p class=\"western\"><span style=\"color: #000000;\"><span>Teradata have specifically implemented </span></span><span style=\"color: #0000ff;\"><u><a href=\"smb://smteam.sas.com/DavWWWRoot/psd/rmi/Implementation%20challenges/TeradataBiTemporal.pdf\"><span>temporal features</span></a></u></span><span style=\"color: #000000;\"><span>, which (interestingly) holds the datetime </span></span><span style=\"color: #000000;\"><span><i>pairs</i></span></span><span style=\"color: #000000;\"><span> in a single column (see attachment). Notice the SAS DDS and Teradata nomenclature differences (for SAS DDS: VALID typically means Transaction Datetimes; for Teradata: VALID refers to Business Datetimes).</span></span></p> <a href=\"/wp-content/uploads/2020/08/hist-op_1.1.6_en_manual.pdf\"><img class=\" aligncenter\" src=\"/wp-content/uploads/2020/08/bt.png\" alt=\"bitemporal\" width=\"900\" height=\"102\" /></a> <p class=\"western\"><span style=\"color: #000000;\"> <span>Furthermore, this SUGI paper ( </span></span><span style=\"color: #0000ff;\"><u><a href=\"http://www2.sas.com/proceedings/sugi29/110-29.pdf\"><span>http://www2.sas.com/proceedings/sugi29/110-29.pdf</span></a></u></span><span style=\"color: #000000;\"><span>) covers the issue. Here is an extract (page 8): </span></span></p> <blockquote> <p class=\"western\"><span style=\"color: #000000;\"><span style=\"font-family: ArialMT, Arial, sans-serif;\"><span style=\"font-size: small;\"><span><b>Versioning history</b></span></span></span></span><span style=\"color: #000000;\"><span style=\"font-family: ArialMT, Arial, sans-serif;\"><span style=\"font-size: small;\"><span> (Type Two style) will always require at least a single </span></span></span></span><span style=\"color: #000000;\"><span style=\"font-family: Arial-BoldMT, Arial Bold, sans-serif;\"><span style=\"font-size: small;\"><span><b>updated date </b></span></span></span></span><span style=\"color: #000000;\"><span style=\"font-family: ArialMT, Arial, sans-serif;\"><span style=\"font-size: small;\"><span>for the record, and two </span></span></span></span><span style=\"color: #000000;\"><span style=\"font-family: Arial-BoldMT, Arial Bold, sans-serif;\"><span style=\"font-size: small;\"><span><b>valid from / valid to dates </b></span></span></span></span><span style=\"color: #000000;\"><span style=\"font-family: ArialMT, Arial, sans-serif;\"><span style=\"font-size: small;\"><span>if using a normalized data model. You will also require two dates in a star schema if past point in-time history queries are to be easily run in a single query.</span></span></span></span></p> <p class=\"western\"><span style=\"color: #000000;\"><span style=\"font-family: ArialMT, Arial, sans-serif;\"><span style=\"font-size: small;\"><span><b>Business history</b></span></span></span></span><span style=\"color: #000000;\"><span style=\"font-family: ArialMT, Arial, sans-serif;\"><span style=\"font-size: small;\"><span> may also dictate a need for </span></span></span></span><span style=\"color: #000000;\"><span style=\"font-family: Arial-BoldMT, Arial Bold, sans-serif;\"><span style=\"font-size: small;\"><span><b>effective from / effective to dates </b></span></span></span></span><span style=\"color: #000000;\"><span style=\"font-family: ArialMT, Arial, sans-serif;\"><span style=\"font-size: small;\"><span>when these may differ from the data warehouse versioning dates. This is especially true in certain sectors, like insurance, where value of business is counted over a period rather than a single time. It is also common when such changes are ‘forward-dated’ in operational systems.</span></span></span></span></p> </blockquote> <p class=\"western\">So – enough of the background – what on earth is “Bitemporal Historisation” anyway?</p> <h2 class=\"western\">Bitemporal Historisation - Overview</h2> <p class=\"western\">Once you ‘get it’, the approach is conceptually very simple. There are essentially just TWO datetime pairs to consider:</p> <p class=\"western\"><b>1 – Transaction datetimes.</b> These from/to datetimes show when the <i>warehouse</i> table is populated. They effectively constitute a ‘version number’ for the data. If we want the latest ‘version’ of data, we query using a ‘high datetime’. If we want the version of data which existed yesterday, we query using yesterday’s date. This datetime-pair provides <i>full auditability</i> of results. Note that 99.999% of the time we will always query using the high datetime (current version, latest transactions). This is a standard SCD type 2 loading process. EVERY table must have these datetimes. The rest of this document will use the term 'Transaction' datetimes, to denote when the record was actually transacted, or committed, to the database.</p> <p class=\"western\"><b>2 – Business datetimes</b>. These from/to datetimes show the period to which the data actually relates. NOT every table will have these datetimes – for many queries we are happy to use the <i>current</i> version of, say, a mapping table, even when producing results for historical periods. The rest of this document will use the term ‘Business’ dates.</p> <h2 class=\"western\">Bitemporal Historisation in Detail</h2> The concept of bitemporal historisation relates to the approach of storing both both business (real world) history alongside transaction (version) history in the same table. This approach aims to achieve the following goals: <ul> <li>Queries always produce the same result, even if the data changes</li> <li>Data changes can be audited. Each change is traceable.</li> <li>Existing queries can be easily adapted by adding temporal ranges to the WHERE clause.</li> <li>Programming overhead is reasonable.</li> <li>Historisation rules can be understood by business users.</li> </ul> <p class=\"western\">The goal of query repeatability is particularly important in regulated environments. In order to repeat results reliably, two time points must always be included in the query. An example might be:</p> <p class=\"western\" align=\"center\">“<i>Which motorcyle coverage did Miss Careful have on April 1st, 2020 as we knew it on April 3rd, 2020?”</i></p> <p class=\"western\">- The first date pair represents a business coverage period. This implies that each coverage must have a <strong>BusinessFrom</strong> and <strong>BusinessTo</strong> datetime to show when the coverage starts and finishes.</p> <p class=\"western\">- The second date represents the point in time at which we made (or would have made) the query. Being a snapshot of the database contents, this is termed the “transaction” date. The rest of this document will use <strong>TransactionFrom</strong> and <strong>TransactionTo</strong> for column names.</p> <p class=\"western\">The above query translated into Bitemporal format might look like this:</p>\n<pre class=\"western\" style=\"padding-left: 30px;\">\nSELECT coverage\nFROM customer_coverage_table AS c\nWHERE c.Contact_LName = 'Fudd'\nAND (c.BusinessFrom le '2020-04-01:00:00:00'dt lt c.BusinessTo)\nAND (c.TransactionFrom le '2020-04-03:00:00:00'dt lt c.TransactionTo);\n</pre> <p class=\"western\">Why aren't we using BETWEEN? Because between is <a href=\"https://sqlblog.org/2011/10/19/what-do-between-and-the-devil-have-in-common#:~:text=See%20the%20full%20index.,range%20%E2%80%93%20not%20everyone%20gets%20that.\">evil</a>!</p> <h2 class=\"western\">Bitemporal Prerequisites and Implications</h2> <p class=\"western\">Implementing a bitemporal approach requires a few principles to be adopted.</p> <h3>Records are Never Modified</h3> <p class=\"western\">With the exception of the TransactionTo datetime field (and maybe the PROCESSED_DTTM in the DDS model), once loaded, a record must <strong>never be modified</strong> (or deleted). This would violate the objective of query repeatability.</p> <h3>Matching Close / Open Dates</h3> When a transaction is closed out and re-opened, or if business values are changing over time, the <em>closing</em> datetime must equal the <em>opening</em> datetime. This is to prevent the \"temporal gap\" that can happen when you close out a record at, say, 23:59:59 and re-open it at 00:00:00. What happens if you query at \"23:59:59.5\" ? The data has disappeared!! Note - not all ETL tools have this capability. It's common for an SCD2 load to add a second, or a day, when opening new records. <h3>Business / Transaction FROM must be less than the TO value</h3> Leading on from the previous point, FROM and TO dates cannot be equal, and it also follows that queries should be formed as follows:\n<pre class=\"western\" style=\"padding-left: 30px;\">\nSELECT *\nFROM sometable as t\nWHERE t.pk = 'some key value'\nAND (t.BusinessFrom le &amp;BUSFROM lt t.BusinessTo)\nAND (t.TransactionFrom le &amp;TXFROM lt c.TransactionTo);\n</pre> The above query would always return either 0 or 1 records. It's imperative that there can only be a single record for a particular key value at a particular point in transaction + business time. <h2 class=\"western\">Simple Bitemporal Examples</h2> <p class=\"western\">Looking at the following (dummy) hierarchy, imagine we first loaded a table on 01JAN2019.</p> <p class=\"western\" lang=\"en-GB\"><img class=\"aligncenter size-full \" src=\"/wp-content/uploads/2020/08/Screenshot-from-2020-08-06-22-56-04.png\" alt=\"bitemporal\" width=\"791\" height=\"89\" /></p> <p class=\"western\">In the first case, consider the ERROR in the country code for XYZ Capital. This was spotted on 8th Feb 2019. The table is updated as follows:</p> <p class=\"western\" lang=\"en-GB\"><img class=\"aligncenter size-full \" src=\"/wp-content/uploads/2020/08/Screenshot-from-2020-08-06-22-53-22.png\" alt=\"\" width=\"790\" height=\"106\" /></p> <p class=\"western\">In the second case, lets consider a business change in NAME from \"Crypto Fund\" to \"Doge GmbH\". The need for this change was raised by the actuaries and performed by the IT team on 4th July 2020. However the actual (legal) change in name occurred on 20<sup>th</sup> April 2020. The data is updated as follows:</p> <img class=\"aligncenter size-full \" src=\"/wp-content/uploads/2020/08/Screenshot-from-2020-08-06-22-50-51.png\" alt=\"\" width=\"794\" height=\"150\" /> In this manner, the previous results can always be reproduced (audited), and an \"up to date\" version of past periods can also be generated. <h2>Complex Bitemporal Example</h2> <p class=\"western\">It can be seen that iterative insertions in the bitemporal model are fairly straightforward, but how will it deal with historical restatements?</p> <p class=\"western\">It is noted that ALL historical restatements deal with the scenario of <i>overlapping ranges</i><i><b>.</b></i><b> </b>The most complex of these is the situation below:</p> <p class=\"western\" lang=\"en-GB\"><a href=\"/wp-content/uploads/2020/08/hist-op_1.1.6_en_manual.pdf\"><img class=\"aligncenter size-full \" src=\"/wp-content/uploads/2020/08/bitemporal5.png\" alt=\"bitmporal overlapping\" width=\"705\" height=\"128\" /></a></p> <p class=\"western\" lang=\"en-GB\">The solution is simply to remove the overlap and create three new records:</p> <p class=\"western\" lang=\"en-GB\"><a href=\"/wp-content/uploads/2020/08/hist-op_1.1.6_en_manual.pdf\"><img class=\"aligncenter size-full \" src=\"/wp-content/uploads/2020/08/bitemporal6.png\" alt=\"bitemporal ranges\" width=\"707\" height=\"228\" /></a></p> <p class=\"western\"><span lang=\"en-GB\">Lets see how this would apply to our data. It is decided by the new CFO on 6th August to temporarily rename </span><span lang=\"en-GB\">\"Trust Us Provincial\" to \"So Very Solvent SA\" for the IFRS17 year end results. Who are we to argue!</span></p> <p class=\"western\" lang=\"en-GB\">The table is dutifully updated as follows:<a href=\"/wp-content/uploads/2020/08/hist-op_1.1.6_en_manual.pdf\"><img class=\"aligncenter size-full \" src=\"/wp-content/uploads/2020/08/Screenshot-from-2020-08-06-22-40-55.png\" alt=\"\" width=\"799\" height=\"213\" /></a></p> <p class=\"western\">We simply query the table (for the natural key “3”) where TechnicalTo equals high date. This gave us 1 record. The new record was inserted ‘from’ 31DEC2019. It applied ‘To’ 01JAN2020. There are now 3 records of business history for the current transaction version of that natural key entry.</p> <h2 class=\"western\">Summary</h2> <p class=\"western\">Bi-temporal historisation can solve many date stamping woes and allow safe modifications to business history without affecting auditability (reproducability) of results. This is far more efficient than taking snapshots of the database, and far easier to work with.</p> SAS does not ship with a Bitemporal transform, however - Data Controller does. It also provides full Data Lineage (forwards &amp; reverse, table &amp; column level, including business logic applied). DDS features such as retained keys, PROCESSED_DTTM columns, and of course - SCD2 is also supported. A Data Catalog, Data Dictionary, and DDL generator are also included. We'd love to assist you in your Data Warehousing project - feel free to\n<p><a href=\"/contact/\">get in touch</a>.</p>","fields":{"slug":"/bitemporal-historisation-and-the-sas-dds/"},"frontmatter":{"title":"Bitemporal Historisation and the SAS DDS","date":"August 06, 2020","author":"Allan Bowe","authorLink":"https://www.linkedin.com/in/allanbowe/","previewImg":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/fd57ce9e2506c53f9552ef76dbd2d6f1/c7fad/Screenshot-from-2020-08-06-22-40-55.png","srcSet":"/static/fd57ce9e2506c53f9552ef76dbd2d6f1/c92b0/Screenshot-from-2020-08-06-22-40-55.png 200w,\n/static/fd57ce9e2506c53f9552ef76dbd2d6f1/3d44d/Screenshot-from-2020-08-06-22-40-55.png 400w,\n/static/fd57ce9e2506c53f9552ef76dbd2d6f1/c7fad/Screenshot-from-2020-08-06-22-40-55.png 799w","sizes":"(min-width: 799px) 799px, 100vw"},"sources":[{"srcSet":"/static/fd57ce9e2506c53f9552ef76dbd2d6f1/8459a/Screenshot-from-2020-08-06-22-40-55.webp 200w,\n/static/fd57ce9e2506c53f9552ef76dbd2d6f1/ab8d1/Screenshot-from-2020-08-06-22-40-55.webp 400w,\n/static/fd57ce9e2506c53f9552ef76dbd2d6f1/26f1a/Screenshot-from-2020-08-06-22-40-55.webp 799w","type":"image/webp","sizes":"(min-width: 799px) 799px, 100vw"}]},"width":799,"height":213}}}}}},{"post":{"html":"<h1>What problem does Data Controller for SAS® solve?</h1>\n<div class=\"imgHolder alignright\"><a href=\"https://www.linkedin.com/in/rgagor/\"><img src=\"/wp-content/uploads/2020/07/IMG-20190430-WA0049.jpg\" alt=\"Rafal Gagor - Veteran SAS Developer\" width=\"180\" height=\"180\" /></a><div><span>Rafal Gagor - Veteran SAS Developer</span></div></div>\n<p>It's a question we get asked a lot, and so this is the first of a series of articles that explore real users and their actual use cases. We caught up with <a href=\"https://www.linkedin.com/in/rgagor/\">Rafal Gagor</a>, a DI Developer with 2 decades of SAS and Financial Services experience, to get his impressions after using Data Controller for SAS on a client project.</p>\n<h2>So, Rafal - what did your Client use Data Controller for?</h2>\n<p>Data Controller was implemented initially as the backbone of a <a href=\"https://sasjs.io\">SASjs</a> Release Management system - it allowed my colleagues and I to upload, for each promote, a list of affected SAS artefacts along with details of the release, and associated JIRA tickets. We could make changes directly via the web interface, or by uploading an Excel file. It was great to capture that information automatically in a database and have data quality rules applied at source. The resultant \"clean\" data enabled the delivery of a robust release management web application that saved hours of manual effort each week.</p>\n<h2>Nice use case. How did you manage before you had Data Controller?</h2>\n<p>Previously, release management was a process performed manually and inconsistently, with data scattered across dozens of Excel and Word documents - it was not brought into SAS at all. In the case of other, regular, business-sourced tables that needed to be uploaded - the options were to either hand-craft an upload process manually as a \"one off\" using Enterprise Guide, or to build (and deploy) an ETL flow sourced from an Excel or a CSV file deployed to a network drive.</p>\n<p>This option was problematic - how frequently to run the flow? What if the file format changed? What if the target table changed? It was therefore quite convenient to have the ability to hand such processes back to the data owner, who could safely modify the data within Data Controller without running the risk of overwriting any indexes or otherwise changing the schema of the table.</p>\n<h2>Last question. What were your favourite Data Controller features?</h2>\n<p>Probably my favourite feature was the <strong>Metadata Navigator</strong> - I hadn't been able to use this since moving away from Base SAS quite some years ago. It was useful to be able to navigate through the objects and associations, and view the properties and attributes, without writing any code. Next up was the <strong>Data Lineage</strong> explorer.</p>\n<p>When the business told me there was an issue with a particular field, it was really helpful to use the Data Controller graphical tools - at both table and column level - to perform a reverse (Target to Source) lineage diagram and quickly understand the data flow. This avoided the need to open up every job in DI Studio and explore the transforms.</p>\n<p>Although it's a basic feature, it was great to use the <strong>Data Viewer</strong> to quickly examine and explore the raw tables without locking the datasets (and hence running the risk of stopping a batch run). The full-table search was a neat touch, as well as the DDL export option. Finally, I liked the fact that there were separate buttons for SUBMIT and APPROVE - a bit like a database where you have to commit the change. It's a nice approach that gives an extra layer of validation for the changes uploaded.</p>\n<h2>Rafal - many thanks!</h2>","fields":{"slug":"/data-controller-developer-perspective/"},"frontmatter":{"title":"Data Controller - a Developer Perspective","date":"July 29, 2020","author":"Allan Bowe","authorLink":"https://www.linkedin.com/in/allanbowe/","previewImg":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#181818","images":{"fallback":{"src":"/static/3795074afaa91c078996114db15428dc/1696a/IMG-20190430-WA0049.jpg","srcSet":"/static/3795074afaa91c078996114db15428dc/cf6cb/IMG-20190430-WA0049.jpg 63w,\n/static/3795074afaa91c078996114db15428dc/7f937/IMG-20190430-WA0049.jpg 125w,\n/static/3795074afaa91c078996114db15428dc/1696a/IMG-20190430-WA0049.jpg 250w","sizes":"(min-width: 250px) 250px, 100vw"},"sources":[{"srcSet":"/static/3795074afaa91c078996114db15428dc/35c53/IMG-20190430-WA0049.webp 63w,\n/static/3795074afaa91c078996114db15428dc/70f7f/IMG-20190430-WA0049.webp 125w,\n/static/3795074afaa91c078996114db15428dc/af4b8/IMG-20190430-WA0049.webp 250w","type":"image/webp","sizes":"(min-width: 250px) 250px, 100vw"}]},"width":250,"height":300}}}}}},{"post":{"html":"<p>End User Computing (EUC) applications are unavoidable - the challenge is not to erase them, but to embrace automated approaches to EUC management that will identify, clean, secure, backup, and integrate EUC data with full auditability, ownership, and approval.</p>\n<h2>The Much-Maligned EUC</h2>\nEUC applications such as Excel, Access Databases, and locally executed programs, are often targeted as the source of a myriad of risks - such as financial misstatements, internal fraud, incorrect models, and potential for business process disruption. The rationale being that business developed / owned applications are not subject to the same access controls, development &amp; testing standards, documentation and release management processes as can be found over the \"IT Fence\". Whilst this is probably true, the inherent flexibility of EUCs that can be quickly updated without service desk requests, project codes, or lost arms &amp; legs - means that EUCs are, regardless, here to stay.\n<p>The challenge is to find a way to shine a light onto this \"Shadow IT\", and provide a framework by which EUC data can be extracted in a simple, safe, secure, scalable, and auditable fashion. <a href=\"/wp-content/uploads/2018/10/DC-UML-Use-Case-Diagram-EUC.png\"><img class=\"aligncenter size-large wp-image-1008\" src=\"/wp-content/uploads/2018/10/DC-UML-Use-Case-Diagram-EUC.png\" alt=\"EUC Use Case Diagram\" /></a></p>\n<h2>EUCs can be Controlled</h2>\nThe 'war on EUCs' cannot be won - it simply isn't practical to ban them, or to migrate / redevelop every closely held and highly complex legacy VBA application. Until alternative solutions for Citizen Developers to build Enterprise Apps (such as <a href=\"https://sasjs.io\">SASjs</a>) become mainstream, simple measures / controls on the EUCs themselves must be implemented - such as version control, readonly attributes, embedded documentation, peer review etc. In the meantime, a management system for EUCs is the ideal place for capturing the requisite metadata needed to monitor, audit, and secure the data therein. Such a management system should have, as a minimum, the following attributes:\n<h3>EUC Data Quality at Source</h3>\nThe ability to run data quality routines at the point of data upload (from EUC to secure IT environment) provides instant feedback to EUC operators that will allow them to make corrections and avoid costly post-upload investigations, re-runs, or worse - incorrect results. As part of this process, it should be easy to create and update those Data Quality rules. A longer discussion of Data Quality can be found <a href=\"https://www.linkedin.com/pulse/zen-art-data-quality-allan-bowe/\">here</a>.\n<h3>EUC Data Review (4 eyes)</h3>\nAfter EUC data is submitted, it should be reviewed before the target database is updated. It should be possible (but not mandatory) for this check to be performed by a different individual. When performing that check, it should only be necessary to review new / changed / deleted records. For changed records, the reviewer should also be able to see the original values. If the data is approved, the target table is updated. If rejected, the staged data can simply be archived.\n<h3>Roles &amp; Responsibilities (RACI)</h3>\nBy capturing who is actually submitting the data, we can see who is responsible for each EUC. By reviewing who is signing off on that data, we have an indication of who is accountable. And by seeing who is being notified of changes to that data, we can deduce who are being consulted / informed. It will then be unnecessary to conduct time-consuming interviews or audits to produce instantly out of date and error-prone EUC ownership documentation!\n<h3>EUC Data Security</h3>\nEUCs are often present on network shares, with opaque access policies and few (if any) controls to prevent unintentional deletion or corruption of data. An EUC management system should ensure data protection from the point of EUC integration right through to the loading of the data to the target table(s). End users should not require write access to the target databases! Neither should individuals in IT be regularly relied upon to run manual scripts for loading business critical data. Finally, it should be possible to restrict (at both column and row level) which groups are given permission to edit or approve data.\n<h3>Ease of Use</h3>\nAdding new tables / EUCs to the system should be a BAU (configuration) task, and possible without needing to secure IT development resource. The process should be so well defined, that new EUC operators can safely integrate their processes with minimum (if any) engagement from IT.\n<h3>EUC Traceability</h3>\nUnderstanding the flow of data into regulatory reports is essential for ensuring the accuracy of the figures they contain. Whilst this can be done automatically in some IT systems (eg SAS Metadata or Prophet Diagram View) the lineage breaks down when data flow crosses system borders. An EUC management system therefore should keep a full history to enable traceback of data items, right back to a copy of the EUC from where the data arrived.\n<h3>EUC Data Integration</h3>\nAny \"system\" worth it's salt will enable easy integration and flexible workflows to ensure that subsequent processes can be triggered on relevant events (such as EUC submission, or data approval). There should be no manual steps other than the act of submitting the data, and reviewing / approving the data.\n<h3>Version control / automated testing</h3>\nThis should really go without saying, however the reality is that there are still many teams (yes, even in IT) who work without source control. Don't even think about building a complex data management system without solid source control and a comprehensive test harness. Not to mention automated build and deployment. When it comes to a system that is responsible for maintenance of business data, it is imperative that it is robust, performant, and filled with checks and controls.\n<h3>Documentation</h3>\nWhilst a decent system should be intuitive enough to operate without a manual, when it comes to maintaining, extending, or using advanced features - documentation is essential, and should be updated regularly. New feature? Write the test, make the fix, build &amp; deploy, pass the test, update the documentation, release. Documentation should be useful for users, developers, and administrators - with diagrams, screenshots, and process flows.\n<h3>Scalability</h3>\nDuring month end, temperatures are high and the pressure is on. The last thing you need on BD2 is system failure, especially when it's 4:30 on a Friday and 150 users are affected. Be sure your platform of choice is proven, supported, and highly available.\n<h3>EUC Auditability</h3>\nOne of the biggest business benefits of an EUC Management System is the ability to trace data directly back to a locked down copy of the EUC that it came from. The system should therefore make it easy to identify and locate that copy, to see who submitted it, who signed it off, and what the precise changes were (adds, updates, deletes). <a href=\"/wp-content/uploads/2018/10/DC-UML-Deployment-Diagram-without-EUC-EUC-version.png\">\n<img class=\"aligncenter wp-image-1055 size-large\" src=\"/wp-content/uploads/2018/10/DC-UML-Deployment-Diagram-without-EUC-EUC-version.png\" alt=\"\" /></a>\n<h2>Data Controller for EUC Management</h2>\n<p>Before you go ahead and build / maintain your own ‘black box’ bespoke EUC reporting solution, take a look at what the Data Controller has to offer (in addition to everything described above):</p>\n<ul>\n<li>Ability to run bespoke SAS programs before / after every edit or approve</li>\n<li>Easy / simple deployment (entirely within your existing SAS platform)</li>\n<li>Roadmap (version restore, data access reports, data profiling)</li>\n<li>A smooth and performant review and approve experience</li>\n<li>A proven methodology for EUC capture</li>\n<li>Extensive <a href=\"https://docs.datacontroller.io\">documentation</a></li>\n<li>Free Community Edition</li>\n<li><a href=\"https://docs.datacontroller.io/excel\">Formula Support</a></li>\n<li>Secured by SAS</li>\n</ul>\n<p>We can also provide an on-site consultant to perform the deployment and user training. <a href=\"/contact\">Get in touch</a> to learn more!</p>\n<div class=\"gatsby-resp-iframe-wrapper\" style=\"padding-bottom: 56.25%; position: relative; height: 0; overflow: hidden; margin-bottom: 1.0725rem\" > <iframe src=\"https://www.youtube-nocookie.com/embed/QhShWNnNjIw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" style=\" position: absolute; top: 0; left: 0; width: 100%; height: 100%; \"></iframe> </div>","fields":{"slug":"/euc-management-system/"},"frontmatter":{"title":"EUC Management Systems need these 12 Attributes","date":"October 30, 2018","author":"Allan Bowe","authorLink":"https://www.linkedin.com/in/allanbowe/","previewImg":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/63132e03fc89ea17c3e019083c4d9b53/9191a/DC-UML-Activity-Diagram-2.png","srcSet":"/static/63132e03fc89ea17c3e019083c4d9b53/1750d/DC-UML-Activity-Diagram-2.png 563w,\n/static/63132e03fc89ea17c3e019083c4d9b53/c5928/DC-UML-Activity-Diagram-2.png 1125w,\n/static/63132e03fc89ea17c3e019083c4d9b53/9191a/DC-UML-Activity-Diagram-2.png 2250w","sizes":"(min-width: 2250px) 2250px, 100vw"},"sources":[{"srcSet":"/static/63132e03fc89ea17c3e019083c4d9b53/cae1a/DC-UML-Activity-Diagram-2.webp 563w,\n/static/63132e03fc89ea17c3e019083c4d9b53/d7a15/DC-UML-Activity-Diagram-2.webp 1125w,\n/static/63132e03fc89ea17c3e019083c4d9b53/81e98/DC-UML-Activity-Diagram-2.webp 2250w","type":"image/webp","sizes":"(min-width: 2250px) 2250px, 100vw"}]},"width":2250,"height":900}}}}}},{"post":{"html":"<p>When applying financial regulations in the EU (such as Solvency II, Basel III or GDPR) it is common for Member States to maintain or introduce national provisions to further specify how such rules might be applied. The National Bank of Belgium (NBB) is no stranger to this, and releases a steady stream of circulars via their <a href=\"https://www.nbb.be/en/financial-oversight/general/news/circulars-and-communications\">website</a>. The <a href=\"https://www.nbb.be/doc/cp/eng/2017/20171012_nbb_2017_27.pdf\">circular</a> of 12th October 2017 (NBB_2017_27, Jan Smets) is particularly interesting as it lays out a number of concrete recommendations for Belgian financial institutions with regard to Data Quality - and stated that these should be applied to internal reporting processes as well as the prudential data submitted. This fact is well known by affected industry participants, who have already performed a self assessment for YE2017 and reviewed documentation expectations as part of the HY2018 submission. <h2>Quality of External Data</h2> The DQ requirements for reporting are described by the 6 <a href=\"https://www.nbb.be/doc/cp/eng/2017/20171012_nbb_2017_27_annex.pdf\">dimensions</a> (Accuracy, Reliability, Completeness, Consistency, Plausibility, Timeliness), as well as the Data Quality Framework described by Patrick Hogan <a href=\"https://www.bankingsupervision.europa.eu/press/conferences/sup_rep_conf/shared/pdf/Item4_1_PatrickHogan.pdf\">here</a> and <a href=\"https://www.bankingsupervision.europa.eu/press/conferences/sup_rep_conf/shared/pdf/2017/Data_quality_framework_tools_and_products.pdf\">here</a>. There are a number of 'hard checks' implemented in OneGate as part of the XBRL submissions, which are kept up to date <a href=\"http://www.eba.europa.eu/risk-analysis-and-data/reporting-frameworks\">here</a>. However, OneGate cannot be used as a validation tool - the regulators will be monitoring the <strong>reliability</strong> of submissions by comparing the magnitude of change between resubmissions! Not to mention the data <strong>plausibility</strong> (changes in submitted values over time). <h2>Data Quality Culture</h2> When it comes to internal processes, CRO's across Belgium must now demonstrate to accredited statutory auditors that they satisfy the 3 Principles of the circular (Governance, Technical Capacities, Process). A long list of action points are detailed - it's clear that a <em>lot</em> of documentation will be required to fulfil these obligations! And not only that - the documentation will need to be continually updated and maintained. It's fair to say that automated solutions have the potential to provide significant time &#x26; cost savings in this regard. <h2>Data Controller for SAS®</h2> The Data Controller is a web based solution for capturing data from users. Data Quality is applied at source, changes are routed through an approval process before being applied, and all updates are captured for subsequent audit. The tool provides evidence of compliance with NBB_2017_27 in the following ways: <h4>Separation of Roles for Data Preparation and Validation (principle 1.2)</h4> Data Controller differentiates between Editors (who provide the data) and Approvers (who sign it off). Editors stage data via the web interface, or by direct file upload. Approvers are then shown the new, changed, or deleted records - and can accept or reject the update. <a href=\"/wp-content/uploads/2018/10/Screen-Shot-2018-10-13-at-22.50.56.png\"><img class=\"aligncenter wp-image-962\" src=\"/wp-content/uploads/2018/10/Screen-Shot-2018-10-13-at-22.50.56.png\" alt=\"\" width=\"553\" height=\"296\" /></a></p>\n<h4>Capacities established should ensure compliance in times of stress (principle 2.1)</h4>\nAs an Enterprise tool, the Data Controller is as scalable and resilient as your existing SAS platform.\n<h4>Capture of Errors and Inconsistencies (principle 2.2)</h4> Data Controller has a number of features to ensure timely detection of Data Quality issues at source (such as cell validation, post edit hook scripts, duplicate removals, rejection of data with missing columns, etc etc). Where errors do make it into the system, a full history is kept (logs, copies of files etc) for all uploads and approvals. Emails of such errors can be configured for follow up. <h4>Tools and Techniques for Information Management Should be Automated (principle 2.3)</h4> The Data Controller can be configured to execute specific .sas programs after data validation. This enables the development of a secure and <em>integrated</em> workflow, and helps companies to avoid the additional documentation penalties associated with \"miscellaneous unconnected computer applications\" and manual information processing. <a href=\"/wp-content/uploads/2018/10/Screen-Shot-2018-10-13-at-22.53.38.png\"><img class=\"aligncenter wp-image-963\" src=\"/wp-content/uploads/2018/10/Screen-Shot-2018-10-13-at-22.53.38.png\" alt=\"\" width=\"278\" height=\"128\" /></a> &nbsp; <h4>Periodic Review &amp; Improvements (principles 2.4 and 3.4)</h4> The Data Controller is actively maintained with the specific aim to reduce the cost of compliance with regulations such as NBB_2017_27. Our <a href=\"https://slides.com/allanbowe/datacontroller/#/\">roadmap</a> includes new features such as pre-canned reports, version 'signoff', and the ability to reinstate previous versions of data. <h4>A process for correction and final validation of reporting before submission (3.1)</h4> As a primary and dedicated tool for data corrections, Data Controller can be described once and used everywhere. <h4>List of Divisions Involved in Preparing Tables (principle 3.2)</h4> By using the Data Controller in combination with knowledge of data lineage (eg from SAS metadata or manual lookup table) it becomes possible to produce an automated report to identify exactly who - and hence which division - was involved in both the preparation and the validation of the all source data per reporting table for each reporting cycle. <h4>Processes should integrate and document key controls (principle 3.3)</h4> Data Controller can be used as a staging point for verifying the quality of data, eg when data from one department must be passed to another department for processing. The user access policy will be as per the existing policy for your SAS environment. <h2>Summary</h2> Whilst the circular provides valuable clarity on the expectations of the NBB, there are significant costs involved to prepare for, and maintain, compliance with the guidance. This is especially the case where reporting processes are disparate, and make use of disconnected EUCs and manual processes. The Data Controller for SAS® addresses and automates a number of pain points as specifically described in the circular. It is a robust and easy-to-use tool, actively maintained and <a href=\"http://docs.datacontroller.io\">documented</a>, and provides an integrated solution on a tried and trusted platform for data management. &nbsp;","fields":{"slug":"/data-quality-and-the-nbb_2017_27-circular/"},"frontmatter":{"title":"Data Quality and the NBB_2017_27 Circular","date":"October 13, 2018","author":"Allan Bowe","authorLink":"https://www.linkedin.com/in/allanbowe/","previewImg":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/d63170332d950a898f89eb83659bd803/737e7/download.png","srcSet":"/static/d63170332d950a898f89eb83659bd803/797b9/download.png 113w,\n/static/d63170332d950a898f89eb83659bd803/834e4/download.png 225w,\n/static/d63170332d950a898f89eb83659bd803/737e7/download.png 450w","sizes":"(min-width: 450px) 450px, 100vw"},"sources":[{"srcSet":"/static/d63170332d950a898f89eb83659bd803/34aa2/download.webp 113w,\n/static/d63170332d950a898f89eb83659bd803/b1e24/download.webp 225w,\n/static/d63170332d950a898f89eb83659bd803/8d211/download.webp 450w","type":"image/webp","sizes":"(min-width: 450px) 450px, 100vw"}]},"width":450,"height":111.99999999999999}}}}}}]}},"pageContext":{"page":"index","archives":{"2018":2,"2020":5,"2021":8,"2022":4,"2023":3},"recentPosts":[{"slug":"/v6-1-source-available/","title":"v6.1 Release: Source Available"},{"slug":"/v6-0-api-explorer/","title":"v6.0 Release: Viya API Explorer"},{"slug":"/v5-3-viewboxes/","title":"v5.3 Release: ViewBoxes"},{"slug":"/v5-2-lineage-updates/","title":"v5.2 Release: Lineage Updates"},{"slug":"/v5-1-library-dataset-info/","title":"v5.1 Release: Library & Dataset Info"},{"slug":"/v5-0-column-level-security/","title":"v5.0 Release: Column Level Security"},{"slug":"/v4-0-formats-special-missings/","title":"v4.0 Release: Formats & Special Missings"},{"slug":"/3-13-extended-data-validation/","title":"v3.13 Release: Extended Data Validation and Native Postgres Support"},{"slug":"/saasnow-partnership/","title":"SaasNow Partnership"},{"slug":"/roi-payback/","title":"ROI and Payback"}],"tags":[{"name":"Releases","totalCount":10},{"name":"SAS","totalCount":8},{"name":"Data Lineage","totalCount":5},{"name":"Data Quality","totalCount":5},{"name":"Excel","totalCount":4},{"name":"Use Cases","totalCount":4},{"name":"Regulatory","totalCount":3},{"name":"Data Catalog","totalCount":2},{"name":"Data Management","totalCount":2},{"name":"EUC","totalCount":2}],"filter":{"fileAbsolutePath":{"regex":"/content/blog/"}},"limit":6,"skip":18,"numPages":4,"currentPage":4}},"staticQueryHashes":["615294906"]}